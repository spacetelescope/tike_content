{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97042b43-59af-4292-aead-081cfbb16f8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec6184-068d-4759-bc0b-35d9f374b245",
   "metadata": {},
   "source": [
    "# Lesson 05: Rotation Rate Statistics\n",
    "\n",
    "## Learning objectives<a name=\"Learningobjectives\"></a>\n",
    "- Fit a Gaussian process to a rotating star's lightcurve.\n",
    "- Understand how to apply Bayesian statistics to astrophysical problems.\n",
    "- Use Bayesian statistics to determine a population-level trend for stellar rotation as a function of flare rate.\n",
    "- Use Python code accelerators to speed up computation.\n",
    "\n",
    "## Introduction\n",
    "In [a previous notebook](../04-flares/04-flares.ipynb), we walked through the steps of inferring a relationship between the rotation rate and flaring rate of an M dwarf population. The method we used to determine the rotation rates — the Lomb-Scargle periodogram — did not produce formal errors. We're left with a few questions, then:\n",
    "\n",
    "- what is the uncertainty on the trend we derived in the previous notebook?\n",
    "- what is the *significance* of the trend we derived in the previous notebook? Can our trend be formally distinguished from a flat line?\n",
    "\n",
    "In this notebook, we will walk through the Bayesian modeling required to answer the above questions. Additionally, we will examine a data-driven approach to modeling stellar rotation: Gaussian processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6639e0-a7c9-459e-9835-71b309e6124e",
   "metadata": {},
   "source": [
    "## Prerequisites<a name=\"prereqs\"></a>\n",
    "- Familiarity with writing functions: https://www.py4e.com/html3/04-functions\n",
    "- [Classifying stellar flares with stella](https://spacetelescope.github.io/hellouniverse/notebooks/hello-universe/Classifying_TESS_flares_with_CNNs/Classifying_TESS_flares_with_CNNs.html)\n",
    "- Previous TIKE notebook: [stellar rotation rates](../04-flares/04-flares.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5580a6-9605-41d1-af69-f29870e7b8f9",
   "metadata": {},
   "source": [
    "## Import Statements<a name=\"import\"></a>\n",
    "\n",
    "* `numpy` is used for array manipulation.\n",
    "* `matplotlib.pyplot` is used to display images and plot datasets.\n",
    "* `lightkurve` allows us to easily interact with TESS light curves.\n",
    "* `seaborn` for generating specialized plots\n",
    "* `numba` to speed up our queries\n",
    "* `celerite`, `pymc`, `arviz` for gaussian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be61f9-7371-4df8-ba3d-960e6cbb841a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manipulating and plotting arrays\n",
    "from matplotlib import ticker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "np.bool = bool # needed to avoid a warning later\n",
    "\n",
    "# querying, reading, plotting data\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "import lightkurve as lk\n",
    "\n",
    "# display nice progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# python accelerator\n",
    "from numba import njit\n",
    "\n",
    "# statistics\n",
    "import scipy \n",
    "\n",
    "# sampling probability distribution\n",
    "import emcee\n",
    "\n",
    "# gaussian process packages\n",
    "from celerite2.pymc import terms, GaussianProcess\n",
    "\n",
    "import pymc as pm\n",
    "import pymc_ext as pmx\n",
    "import pytensor.tensor as tt\n",
    "import arviz as az\n",
    "import corner\n",
    "\n",
    "# enable cloud queries\n",
    "import s3fs\n",
    "Observations.enable_cloud_dataset()\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "import os\n",
    "# inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453f9e0-82df-4e73-8dda-95b6518b3256",
   "metadata": {},
   "source": [
    "# How sinusoidal are rotating stars' light curves?<a name=\"sinusoidal\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e737cfc-ca15-4b07-8597-afca3311277f",
   "metadata": {},
   "source": [
    "In a [previous notebook](../../07-stellar-rotation), we've assessed the periodic nature of rotating stars. As mentioned above, doing so with the Lomb-Scargle periodogram did not yield error bars on the calculated periods, limiting the statistical inference that we could perform.\n",
    "\n",
    "Another consideration with the Lomb-Scargle periodogram is that is assumes a sinusoidal form to the underlying periodic signal. Let's take a closer look at our rotating stars to determine how warranted this assumption is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f2c6f-0aea-4b03-8667-4324469191fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ticids = ['234295610']\n",
    "\n",
    "# query for results, filter for light curve files\n",
    "obs = Observations.query_criteria(target_name=ticids, provenance_name=\"SPOC\", sequence_number=[1])\n",
    "prod = Observations.get_product_list(obs)\n",
    "filt = Observations.filter_products(prod, description=\"Light curves\")\n",
    "c_uris = Observations.get_cloud_uris(filt)\n",
    "\n",
    "# Initialize a TESS lightCurve in lightKurve\n",
    "with fs.open(c_uris[0], \"rb\") as f:\n",
    "    with fits.open(f, \"readonly\") as fts:\n",
    "        lc = lk.TessLightCurveFile(fts)\n",
    "    \n",
    "lc.plot()\n",
    "plt.xlim(1325, 1335)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78880a8b-3c95-4c15-9876-54332105db03",
   "metadata": {},
   "source": [
    "Ignoring the flares in the dataset, there's definitely some periodic sub-structure. But is it strictly sinusoidal? Let's overplot a sinusoid, using the period we derived for this star in a [previous notebook](../../07-interm_rotation_rate). For the amplitude of the sine curve, we'll use the standard deviation of the light curve as a reasonable scale. Similarly, for the offset of the sine curve, we'll use the median value of the light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7f5d5-e095-43dc-8a0b-e0eeb54859e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "period_guess = 0.76 # from previous notebook\n",
    "\n",
    "x = np.linspace(1325, 1335, 400)\n",
    "y = np.median(lc.flux.value) + np.std(lc.flux.value)*np.sin(2 * np.pi  * x / period_guess)\n",
    "\n",
    "lc.plot()\n",
    "plt.plot(x,y)\n",
    "plt.xlim(1325, 1335)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25e67f-5a6c-413a-9710-7d9411380c29",
   "metadata": {},
   "source": [
    "It looks like our previously derived period agrees with the periodicity reasonably well — at least by eye. But it's clear that there are clear departures from a sine wave in our data. The peaks in our sine wave coincide with periodic peaks in our light curve, but there is a secondary peak after the first peak in each cycle in the light curve. Our sinusoid model, by nature, cannot capture this secondary peak.\n",
    "\n",
    "Worse yet, the \"period\" of this star is not strictly periodic. As described in [other notebooks](../../07-interm_rotation_rate), we can assess the rotation rate of a star because of its spotted surface: as spots rotate in and out of view, the amount of light that the observer receives proportionally varies. But spots don't live forever. Because the spot distribution on a stellar surface can therefore change, a rotating star's light curve is \"quasi-periodic\" (e.g., <a href=\"https://www.aanda.org/articles/aa/abs/2011/03/aa15877-10/aa15877-10.html\">Dumusque et al. 2011</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9a2d0-9ec2-43f7-bbba-9c7e7a8c700e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gaussian processes and rotating stars<a name=\"gps\"></a>\n",
    "The above limitations motivate using an alternative to the Lomb-Scargle periodogram for identifying a correlation between rotation rate and flare rate. One useful tool in this domain is a Gaussian process ([Rasmussen & Williams 05](https://link.springer.com/content/pdf/10.1007/b100712.pdf#page=71), [Angus+19](https://academic.oup.com/mnras/article/474/2/2094/4209242), [Aigrain & Foreman-Mackey 23](https://www.annualreviews.org/doi/abs/10.1146/annurev-astro-052920-103508)). \n",
    "\n",
    "Gaussian processes can be thought of as an extension of the multi-variate Gaussian distribution. The multivariate Gaussian distribution is defined by a mean *vector* and a *covariance matrix*. The mean vector is a collection of numbers that describe the mean value of the distribution in each of its multiple dimensions, and the covariance matrix describes how the random variables in each dimension are related to each other. Gaussian processes step beyond this: they are described by a mean *function* and a *covariance function*. In our science case, we're interested in the (quasi-periodic) correlation between points — so we want to recover the covariance function underlying a dataset.\n",
    "\n",
    "This is a bit abstract, so let's put things into practice to gain some intuition. To do so, we'll use Gaussian processes via the [exoplanet package](https://gallery.exoplanet.codes/tutorials/stellar-variability/). First, we'll load in the rotation rates and flare rates from a [previous notebook](../../07-interm_rotation_rate) as calculated with Lomb-Scargle. We'll use the Lomb-Scargle rates as an initial guess for the Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b166ee-1e5b-4374-91cf-07151e93000a",
   "metadata": {},
   "source": [
    "First, we need to clean our dataset. We'll remove the 3 sigma outliers, and we'll grab only the first 5,000 points of our light curve. We do this to reduce the computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cc80c-7d8a-48b1-add2-ce1ea199ad2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove nans, outliers, trim light curve so we have less data to analyze\n",
    "lc2 = lc.remove_nans().remove_outliers()\n",
    "lc2 = lc2[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ccaf3-5b86-4213-91e9-21426f594f37",
   "metadata": {},
   "source": [
    "Next, we'll recast the light curve quantities that we care about — the flux and the time stamps — into nicely formatted numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9101a-a54a-4fbc-98ef-4a1c5207fb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.ascontiguousarray(lc2.time.value, dtype=np.float64)\n",
    "y = np.ascontiguousarray(lc2.flux, dtype=np.float64)\n",
    "yerr = np.ascontiguousarray(lc2.flux_err, dtype=np.float64)\n",
    "mu = np.mean(y)\n",
    "\n",
    "# center the flux around 0 for probabilistic modeling\n",
    "y = (y / mu - 1) * 1e3\n",
    "\n",
    "# perform error propagation\n",
    "yerr = yerr * 1e3 / mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebf502-fdfc-444a-9845-d0887a678ecf",
   "metadata": {},
   "source": [
    "Now it's time to dig into our Gaussian Process! First, we define the probabilistic model. We'll do so using the created our first [PyMC](https://www.pymc.io/welcome.html) framework. PyMC is a framework used for probabilistic modeling. We'll be using its `model` object to contain random variables in our investigation.\n",
    "\n",
    "The entire model needs to be contained within a single `with` block. This process creates a local scope for `PyMC`, allowing it to create a graph connecting random variables.\n",
    "\n",
    "Once the random variable container — the *model* — is ready, we now define the mean function. This function is just a Gaussian centered around 0. We're essentially making the assumption here that there is no underlying astrophysics — no signal — that we're trying to model aside from the covariance between points. \n",
    "\n",
    "`mean = pm.Normal(\"mean\", mu=0.0, sigma=10.0)`\n",
    "\n",
    "Next, we can add a \"jitter\" term. This term allows the model to account for more white noise than described by error bars. Every time we include a new term in this framework, we specify its *prior* distribution — that is, the distribution that encodes our understanding of that parameter's value before doing our Bayesian calculation.\n",
    "\n",
    "`log_jitter = pm.Normal(\"log_jitter\", mu=np.log(np.mean(yerr)), sigma=2.0)`\n",
    "\n",
    "We can next get started actually defining the *kernel* (another word for the covariance function) of the Gaussian Process. We'll include two terms: a simple harmonic oscillator term (`SHOTerm`) and a rotation term (`RotationTerm`). The `SHOTerm` takes three arguments: sigma (the standard deviation of the process), rho (the undamped period of the oscillator), and Q (related to the damping timescale of the process). We'll define sigma and rho probabilistically so that the model can infer their values from the data. You can learn more about the `SHOTerm` in the [celerite2 documentation](https://celerite2.readthedocs.io/en/latest/api/python/) and [paper](https://iopscience.iop.org/article/10.3847/1538-3881/aa9332).\n",
    "\n",
    "~~~\n",
    "\n",
    "sigma = pm.InverseGamma(\n",
    "        \"sigma\", **pmx.utils.estimate_inverse_gamma_parameters(1.0, 5.0)\n",
    "    )\n",
    "\n",
    "\n",
    "rho = pm.InverseGamma(\n",
    "    \"rho\", **pmx.utils.estimate_inverse_gamma_parameters(0.5, 2.0)\n",
    ")\n",
    "~~~\n",
    "\n",
    "With the `SHOTerm` inputs defined, we can set up the `RotationTerm`. This probabilistic term will tell us about the rotation of the star itself. We'll once more need to define a sigma of this distribution, in addition to a log period, a log Q0 (the quality factor for the secondary oscillation, which describes how damped it is), a log dQ (difference in quality factors of the first and second modes), and an f term (the fractional amplitude of the secondary mode with respect to the first). \n",
    "\n",
    "~~~\n",
    "sigma_rot = pm.InverseGamma(\n",
    "        \"sigma_rot\", **pmx.utils.estimate_inverse_gamma_parameters(1.0, 5.0)\n",
    "    )\n",
    "log_period = pm.Normal(\"log_period\", mu=np.log(0.76), sigma=2.0)\n",
    "period = pm.Deterministic(\"period\", tt.exp(log_period))\n",
    "log_Q0 = pm.HalfNormal(\"log_Q0\", sigma=2.0)\n",
    "log_dQ = pm.Normal(\"log_dQ\", mu=0.0, sigma=2.0)\n",
    "f = pm.Uniform(\"f\", lower=0.1, upper=1.0)\n",
    "~~~\n",
    "\n",
    "Finally, we can add the `SHOTerm` and the `RotationTerm`, calculate the likelihood, and optimize the model.\n",
    "\n",
    "~~~\n",
    "gp = GaussianProcess(\n",
    "        kernel,\n",
    "        t=x,\n",
    "        diag=yerr**2 + tt.exp(2 * log_jitter),\n",
    "        mean=mean,\n",
    "        quiet=True,\n",
    "    )\n",
    "\n",
    "    # Compute the Gaussian Process likelihood and add it into the\n",
    "    # the PyMC3 model as a \"potential\"\n",
    "    gp.marginal(\"gp\", observed=y)\n",
    "\n",
    "    # Compute the mean model prediction for plotting purposes\n",
    "    pm.Deterministic(\"pred\", gp.predict(y))\n",
    "\n",
    "    # Optimize to find the maximum a posteriori parameters\n",
    "    map_soln = pmx.optimize(progress=True)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec3056-37a1-420f-b417-f3285622df20",
   "metadata": {},
   "source": [
    "Let's see this all play out in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a538df4-cb5a-4e80-a55f-d1b0371e8f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # The mean function.\n",
    "    mean = pm.Normal(\"mean\", mu=0.0, sigma=10.0)\n",
    "\n",
    "    # A jitter term describing excess white noise\n",
    "    log_jitter = pm.Normal(\"log_jitter\", mu=np.log(np.mean(yerr)), sigma=2.0)\n",
    "\n",
    "    # A term to describe the non-periodic variability\n",
    "    sigma = pm.InverseGamma(\n",
    "        \"sigma\", **pmx.utils.estimate_inverse_gamma_parameters(1.0, 5.0)\n",
    "    )\n",
    "    rho = pm.InverseGamma(\n",
    "        \"rho\", **pmx.utils.estimate_inverse_gamma_parameters(0.5, 2.0)\n",
    "    )\n",
    "\n",
    "    # The parameters of the RotationTerm kernel\n",
    "    sigma_rot = pm.InverseGamma(\n",
    "        \"sigma_rot\", **pmx.utils.estimate_inverse_gamma_parameters(1.0, 5.0)\n",
    "    )\n",
    "    log_period = pm.Normal(\"log_period\", mu=np.log(0.76), sigma=2.0)\n",
    "    period = pm.Deterministic(\"period\", tt.exp(log_period))\n",
    "    log_Q0 = pm.HalfNormal(\"log_Q0\", sigma=2.0)\n",
    "    log_dQ = pm.Normal(\"log_dQ\", mu=0.0, sigma=2.0)\n",
    "    f = pm.Uniform(\"f\", lower=0.1, upper=1.0)\n",
    "\n",
    "    # Set up the Gaussian Process model\n",
    "    kernel = terms.SHOTerm(sigma=sigma, rho=rho, Q=1 / 3.0)\n",
    "    kernel += terms.RotationTerm(\n",
    "        sigma=sigma_rot,\n",
    "        period=period,\n",
    "        Q0=tt.exp(log_Q0),\n",
    "        dQ=tt.exp(log_dQ),\n",
    "        f=f,\n",
    "    )\n",
    "    gp = GaussianProcess(\n",
    "        kernel,\n",
    "        t=x,\n",
    "        diag=yerr**2 + tt.exp(2 * log_jitter),\n",
    "        mean=mean,\n",
    "        quiet=True,\n",
    "    )\n",
    "\n",
    "    # Compute the Gaussian Process likelihood and add it into the\n",
    "    # the PyMC3 model as a \"potential\"\n",
    "    gp.marginal(\"gp\", observed=y)\n",
    "\n",
    "    # Compute the mean model prediction for plotting purposes\n",
    "    pm.Deterministic(\"pred\", gp.predict(y))\n",
    "\n",
    "    # Optimize to find the maximum a posteriori parameters\n",
    "    map_soln = pmx.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff6842-e6d0-43df-9495-4b3223522125",
   "metadata": {},
   "source": [
    "We've successfully fit our first Gaussian Process! We can ignore the last error.\n",
    "\n",
    "Let's plot the best-fitting solution against our data to see how will things match by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721b469-cfce-43f5-a94a-a733e6533df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, \"k\", label=\"data\")\n",
    "plt.plot(x, map_soln[\"pred\"], color=\"C1\", label=\"model\")\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel(\"time [days]\")\n",
    "plt.ylabel(\"relative flux [ppt]\")\n",
    "plt.title(\"KIC 5809890; map model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb937c0-71b3-4969-bada-7cf82cce7846",
   "metadata": {},
   "source": [
    "Nice! We're clearly capturing the non-sinusoidal variability that our sinusoid model could not. How did it actually do that, though?\n",
    "\n",
    "To fit this model, the code optimized something known as the *posterior distribution*. Before we get into posterior distributions, let's talk about the likelihood.\n",
    "\n",
    "In statistics, the likelihood function is a way of quantifying how well a model's parameters describe a given dataset. This tells us the probability of observing the provided dataset with a set of model parameters. As an example common likelihood function (and one that has a deep relationship to least-squares fitting; e.g., [Hogg+10](https://arxiv.org/abs/1008.4686)) is the likelihood assuming Gaussian errors:\n",
    "\n",
    "$\\mathcal{L} = \\sum(\\frac{(y_i - f(\\theta)_i))^2}{\\sigma_i^2} + \\log{2\\pi\\sigma_i})$,\n",
    "\n",
    "![Illustration of Bayes Theorem. The prior distribution is a Gaussian toward the left of the figure, and the likelihood is to the right. The posterior distribution lies in the middle.](bayes_sketch.png \"Title\")\n",
    "\n",
    "\n",
    "where $\\mathcal{L}$ is the likelihood, $y_i$ is an observed data point, $f$ is the model, $\\theta$ are the parameters, and $\\sigma_i$ are the uncertainties. The Gaussian prior likelihood that we just optimized generalizes something like this to function, instead of values.\n",
    "\n",
    "With our likelihood in hand, we can next ask ourselves what the probability of our model is given the data. We can use Bayes' theorem to help us out:\n",
    "\n",
    "$p(\\theta|y) \\propto p(x|\\theta)p(\\theta)$.\n",
    "\n",
    "\n",
    "\n",
    "In plain words, the above equation reads: \"The probability of these parameters given the data is proportional to the probability of the data given these parameters multiplied by the probability of these parameters.\" \n",
    "\n",
    "The likelihood is equivalent to the second term in the expression, the probability of the data given our parameters, so we substitute it in:\n",
    "\n",
    "$p(\\theta|y) \\propto \\mathcal{L}p(\\theta)$.\n",
    "\n",
    "So, to infer our posterior distribution — the probability of our parameters given our data, *after* performing our Bayesian calculation — we simply multiply our likelihood by our prior distribution, which we discussed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c0424-aff1-4f53-b2ae-1078539db120",
   "metadata": {},
   "source": [
    "# Sampling the posterior distribution<a name=\"sampling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43505bf4-69b0-4b23-92a9-db19c4ff6b00",
   "metadata": {},
   "source": [
    "The above exercise has only told us the \"maximum a posteriori model prediction\" — which can be thought of as the best-fitting model, identifying which parameters produce the peak in the posterior distribution. But one big improvement that we wanted over the Lomb-Scargle periodogram was principled uncertainties on the period. We can calculate principled uncertainties in the Gaussian Process framework by *statistically sampling* the posterior. Essentially, this means that we will explore the likelihood function so that we can see how wide the peak is.\n",
    "\n",
    "Ideally, we'd just calculate the full likelihood function so that we could fully flesh out all of its peaks. However, in a lot of astrophysical examples — like ours — doing that would take a really long time. So, what we do instead is use a sampler to efficiently explore the likelihood. This efficient exploration allows us to understand the shape of the posterior distribution without performing too many Gaussian Process calculations.\n",
    "\n",
    "Luckily, we won't have to get our hands too dirty with statistical sampling yet, because pymc will do it for us. We can use the ``fit`` function to perform variational inference — a type of inference that is faster than direct sampling, at the cost of assuming that posterior distributions are like Gaussians.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad03467-febe-4308-b3a7-9e28cf99cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.fit(\n",
    "        progressbar=True,\n",
    "        start=map_soln\n",
    "    )\n",
    "approx_sample = trace.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c3455-64cf-426d-9142-b93583541800",
   "metadata": {},
   "source": [
    "With our sampling complete, we can use the [arviz](https://python.arviz.org/en/stable/) package to see the marginal distributions for each parameter. That is, we can see the distribution of samples for each individual parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d164e-6197-4298-ba12-ce763024badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(\n",
    "    approx_sample,\n",
    "    var_names=[\n",
    "        \"f\",\n",
    "        \"log_dQ\",\n",
    "        \"log_Q0\",\n",
    "        \"log_period\",\n",
    "        \"sigma_rot\",\n",
    "        \"rho\",\n",
    "        \"sigma\",\n",
    "        \"log_jitter\",\n",
    "        \"mean\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065540c-2687-471a-b0c5-1ead985d3d9c",
   "metadata": {},
   "source": [
    "The `mean` and `sd` columns describe the mean and standard deviation of the sampled distributions for each parameter. \n",
    "\n",
    "The other columns are a bit less intuitive. \n",
    "- `hdi_3%` and `hdi_97%` describe the values below which only 3% of samples lie and abeove with only 3% of samples lie, respectively. These parameters essentially tell you the spread of your samples.\n",
    "- `mcse_mean` and `mcse_sd` are the Monte Carlo standard error on the mean and the Monte Carlo standard error on the standard deviation. These parameters essentially tell you how precise the mean and standard deviations of your parameters are, and therefore whether they might change between different simulations. Low values for these parameters are in general better. In our case, the `mcse_mean` and `mcse_sd` are much smaller than the mean and standard deviation of the parameter we care about — the log rotation period — so our sampling seems to have gone well.\n",
    "- `ess_bulk` and `ess_tail` are used to estimate the quality of the samping, in terms of how many independent samples were produced. In principle, we want all samples to be independent draws from the posterior distribution. But many sampling techniques require different chains *walking around*, so some information about the previous step is baked into the position of the current step. These estimates describe the effective sample size in the bulk of the samples (the more dense region) and the tail of the samples (farther away from the central, dense region).\n",
    "- finally, `r_hat` (also known as the Gelman-Rubin statistic) can be used to assess whether the sampler has converged. Generally, we want r_hat values less than 1.2 for convergence. Because we don't do direct sampling in this step, our `r_hat` values are simply ``NaN``s.\n",
    "\n",
    "todo: remove some of these columns? we don't use them later, but we could dig into them in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7091c-e41c-4c82-8b43-680e97dbddd0",
   "metadata": {},
   "source": [
    "Let's plot the samples for our sole parameter of interest: the stellar rotation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cac8f4-f2be-46b1-b1dc-11b057c9402a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "period_samples = np.asarray(approx_sample.posterior[\"period\"]).flatten()\n",
    "plt.hist(period_samples, 25, histtype=\"step\", color=\"k\", density=True, label='GP Samples')\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"log(rotation period [days])\")\n",
    "plt.axvline(np.log(0.76), color='teal', linestyle='--', lw=2, label='Lomb-Scargle period estimate')\n",
    "_ = plt.ylabel(\"posterior density\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858ee21-39bd-4803-9aa4-9db2089e8418",
   "metadata": {},
   "source": [
    "Wow! It looks like Lomb-Scargle gave us a pretty different answer than the Gaussian process result. Additionally, it looks like the sampler has found significant uncertainty in the inferred rotation rate of this star. To correctly determine a relationship between rotation rate and flare rate, this uncertainty must be properly accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a19f6-f381-40a0-b1d4-99d7220bbad9",
   "metadata": {},
   "source": [
    "# Gaussian processes for the whole population<a name=\"gaussian_pop\"></a>\n",
    "Now that we've done this step for one star, let's wrap it in a function and do it for all the stars in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f605a5-eef6-41fc-8be2-39b0ec8fc4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_rotation_gp(name, period_init):\n",
    "    # query for results, filter for light curve files\n",
    "    obs = Observations.query_criteria(target_name=ticids, provenance_name=\"SPOC\", sequence_number=[1,2])\n",
    "    prod = Observations.get_product_list(obs)\n",
    "    filt = Observations.filter_products(prod, description=\"Light curves\")\n",
    "    c_uris = Observations.get_cloud_uris(filt)\n",
    "\n",
    "    # Initialize a TESS lightCurve in lightKurve\n",
    "    with fs.open(c_uris[0], \"rb\") as f:\n",
    "        with fits.open(f, \"readonly\") as fts:\n",
    "            lc = lk.TessLightCurveFile(fts)\n",
    "    \n",
    "    lc2 = lc.remove_nans().remove_outliers()\n",
    "    lc2 = lc2[:5000]\n",
    "\n",
    "    x = np.ascontiguousarray(lc2.time.value, dtype=np.float64)\n",
    "    y = np.ascontiguousarray(lc2.flux, dtype=np.float64)\n",
    "    yerr = np.ascontiguousarray(lc2.flux_err, dtype=np.float64)\n",
    "    mu = np.mean(y)\n",
    "    y = (y / mu - 1) * 1e3\n",
    "    yerr = yerr * 1e3 / mu\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # The mean flux of the time series\n",
    "        mean = pm.Normal(\"mean\", mu=0.0, sigma=10.0)\n",
    "\n",
    "        # A jitter term describing excess white noise\n",
    "        log_jitter = pm.Normal(\"log_jitter\", mu=np.log(np.mean(yerr)), sigma=2.0)\n",
    "\n",
    "        # A term to describe the non-periodic variability\n",
    "        sigma = pm.InverseGamma(\n",
    "            \"sigma\", **pmx.utils.estimate_inverse_gamma_parameters(1.0, 5.0)\n",
    "        )\n",
    "        rho = pm.InverseGamma(\n",
    "            \"rho\", **pmx.utils.estimate_inverse_gamma_parameters(0.5, 2.0)\n",
    "        )\n",
    "\n",
    "        # The parameters of the RotationTerm kernel\n",
    "        sigma_rot = pm.InverseGamma(\n",
    "            \"sigma_rot\", **pmx.utils.estimate_inverse_gamma_parameters(1.0, 5.0)\n",
    "        )\n",
    "        log_period = pm.Normal(\"log_period\", mu=np.log(period_init), sigma=2.0)\n",
    "        period = pm.Deterministic(\"period\", tt.exp(log_period))\n",
    "        log_Q0 = pm.HalfNormal(\"log_Q0\", sigma=2.0)\n",
    "        log_dQ = pm.Normal(\"log_dQ\", mu=0.0, sigma=2.0)\n",
    "        f = pm.Uniform(\"f\", lower=0.1, upper=1.0)\n",
    "\n",
    "        # Set up the Gaussian Process model\n",
    "        kernel = terms.SHOTerm(sigma=sigma, rho=rho, Q=1 / 3.0)\n",
    "        kernel += terms.RotationTerm(\n",
    "            sigma=sigma_rot,\n",
    "            period=period,\n",
    "            Q0=tt.exp(log_Q0),\n",
    "            dQ=tt.exp(log_dQ),\n",
    "            f=f,\n",
    "        )\n",
    "        gp = GaussianProcess(\n",
    "            kernel,\n",
    "            t=x,\n",
    "            diag=yerr**2 + tt.exp(2 * log_jitter),\n",
    "            mean=mean,\n",
    "            quiet=True,\n",
    "        )\n",
    "\n",
    "        # Compute the Gaussian Process likelihood and add it into the\n",
    "        # the PyMC3 model as a \"potential\"\n",
    "        gp.marginal(\"gp\", observed=y)\n",
    "\n",
    "        # Compute the mean model prediction for plotting purposes\n",
    "        pm.Deterministic(\"pred\", gp.predict(y))\n",
    "\n",
    "        # Optimize to find the maximum a posteriori parameters\n",
    "        map_soln = pmx.optimize()\n",
    "        \n",
    "    with model:\n",
    "        trace = pmx.utils.sample(\n",
    "            tune=1000,\n",
    "            draws=1000,\n",
    "            start=map_soln,\n",
    "            cores=4,\n",
    "            chains=2,\n",
    "            target_accept=0.9,\n",
    "            return_inferencedata=True,\n",
    "            random_seed=[10863087, 10863088],\n",
    "            progressbar=False\n",
    "        )\n",
    "    \n",
    "    period_samples = np.asarray(approx_sample.posterior[\"period\"]).flatten()\n",
    "    np.save(f'period_samples{name}.npy', period_samples)\n",
    "    return period_samples\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda933a-08a8-40ce-8c71-680ecf9accf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = np.loadtxt('stars.txt')\n",
    "rot_rates_constrained = np.loadtxt('rot_rates_constrained.txt')\n",
    "flare_rates_durations = np.loadtxt('flare_rates_durations.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f20896-51a4-45ee-a557-603452a0b02b",
   "metadata": {},
   "source": [
    "The below shows how to loop through the stars and compute the rotation rate with a Gaussian process. This takes far too long, though — feel free to skip over the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30245228-7d1c-4e68-bd64-b9b18a942f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rot_rates_gp = []\n",
    "\n",
    "# iterate though stars\n",
    "rot_rates_gp = []\n",
    "for i, star in tqdm(enumerate(stars[::200]), total=len(stars[::200])):\n",
    "    \n",
    "#     flare_rates_duration = flare_rates_durations[i]\n",
    "#     # only fit the stars with \"good\" flare rates.\n",
    "#     if flare_rates_duration<=0.0:\n",
    "#         rot_rates_gp += [0.0]\n",
    "#     star = int(star)\n",
    "#     star_name = str(star) # the rotation rate function takes strings\n",
    "#     period_guess = rot_rates_constrained[i]\n",
    "#     if os.path.exists(f'period_samples{star_name}.npy'):\n",
    "#         continue\n",
    "    \n",
    "#     rot_rate_gp_samples = fit_rotation_gp(star_name, period_guess)\n",
    "    \n",
    "#     # store result of rotation rate calculation\n",
    "#     rot_rates_gp += [rot_rate_gp_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c944d1-fe45-437b-87b2-5de0c936a7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if loading back in after performing the computations\n",
    "rot_rates_gp = np.load('all_rot_rates.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b4bbe-0b71-4e00-b750-e985dfcc9fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(rot_rates_gp[0], label='TIC ' + str(int(stars[0])), histtype=\"step\", color=\"teal\", density=False)\n",
    "plt.hist(rot_rates_gp[1], label='TIC ' + str(int(stars[1])), histtype=\"step\", color=\"goldenrod\", density=False)\n",
    "plt.hist(rot_rates_gp[-1], label='TIC ' + str(int(stars[-1])),\n",
    "         histtype=\"step\", color=\"k\", density=False)\n",
    "\n",
    "plt.xlabel('Period (days)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e213080-6dbe-4c05-ac61-3f27ba850c22",
   "metadata": {},
   "source": [
    "It looks like there's pretty significant spread in the distributions of periods and the associated scales. Some rotation rates are very well constrained, whereas others have much wider uncertainty. This result makes it even clearer that we have to take the period uncertainties into account when looking for a population-level trend. For example, if one period value is very uncertain, we want it to influence our trend less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003dc1f-9a41-40cc-bffc-5ded338b5f60",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian modeling<a name=\"hierarchical\"></a>\n",
    "\n",
    "The problem at this stage is that we want to draw a line through period vs. flare rate and understand the uncertainty on that line, given uncertainties on period. But the period uncertainties are very non-trivial; note that the above-plotted distributions don't necessarily look Gaussian. How should we proceed?\n",
    "\n",
    "The idea is to use a technique called hierarchical Bayesian modeling. This technique is used to model relationships between different levels of structured data, all of which have their own associated parameters and uncertainties. In our case, hierarchical Bayesian modeling allows us to infer, all at once and self-consistently, the uncertainties on the individual rotation rates and uncertainties on the overall trend that describes those rotation rates as a function of flare rate.\n",
    "\n",
    "At this point, we've already gone through all the trouble of sampling the likelihood for each star to infer its rotation rate posterior distribution. Luckily, we don't have to perform a new sampling calculation from scratch. Using the statistical formulation of [Hogg et al. 2010](https://iopscience.iop.org/article/10.1088/0004-637X/725/2/2166/meta) (as also implemented in, e.g., [Lustig-Yaeger et al. 2022](https://iopscience.iop.org/article/10.3847/1538-3881/ac5034/meta)), we can use the samples we've already generated at the individual level (for each star) to perform our population-level sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af390a-009d-4e57-b4d5-3ab164cdddcb",
   "metadata": {},
   "source": [
    "Now it's time to perform our hierarchical Bayesian sampling, following the prescription of [Lustig-Yaeger et al. 2022](https://iopscience.iop.org/article/10.3847/1538-3881/ac5034/meta). Note that because the likelihood function contains a nested loop, we get a few orders of magnitude speed-up by using the Numba package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d19744-e41c-43c0-be40-9edf203d17bf",
   "metadata": {},
   "source": [
    "First, we'll define the hyperprior. This function tells us the prior distribution on our hyperparameters — in our case, the slope and intercept of the trend line we're fitting. All of these samples are accelerated with the `numba` package. After a function's first exection, the `@njit` decorator compiles the function down machine code to make the code faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c575b40-1245-4e55-94b4-794cf496d68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def hyperprior(hypertheta):\n",
    "    \"\"\"\n",
    "    Prior for the population-level trend describing the relationship between (log) flare rate and (log) rotation rate.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "        :hypertheta: (array) parameters for the population-level trend.\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "        :hyperprior_val: (float) value of the prior.\n",
    "    \"\"\"\n",
    "    \n",
    "    # below is a \"top hat\" prior. Anything outside of the boundary returns negative infinity, and anything inside returns 0.\n",
    "    m, b, sigma_2 = hypertheta\n",
    "    if not (-7 < m < 7 and -10 < b < 10 and sigma_2 > 0.2):\n",
    "        return -np.inf\n",
    "\n",
    "    \n",
    "    # this is a Gaussian prior. We want the value to be close to 0, so that the line is rewarded for being close to the data.\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "    return np.log(1.0 / (np.sqrt(2 * np.pi) * sigma)) - 0.5 * (sigma_2 - mu)**2 / sigma**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f843a-3ec1-4d7c-9c13-dd127d0202fe",
   "metadata": {},
   "source": [
    "Note that we've also included a `sigma_2` parameter. This parameter basically penalizes trend lines that are too far away from the data, making sure that it doesn't stray too far away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559ecb5-c969-46a6-85d3-bbc34e9f3a02",
   "metadata": {},
   "source": [
    "Next, we define `f_rotation` — the population-level trend that we're fitting. In our case, we're fitting a linear trend line to relate the log rotation rate to the log flare rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f1dff-3be8-4748-b665-1d2890061628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def f_rotation(m, b, flare): # just a log-linear functional form\n",
    "    \"\"\"\n",
    "    this is the function that returns the LOG rotation rate. it encodes the population trend.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "        :m: (float) slope of trend.\n",
    "        :b: (float) intercept of trend.\n",
    "        :flare: (float) flare rate.\n",
    "    \"\"\"\n",
    "    return m * np.log10(flare) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd5e7c-117a-4a2b-a793-f710657c6fe0",
   "metadata": {},
   "source": [
    "To perform our hierarchical sampling, we'll also need to utilize the previous prior used to perform our individual-star sampling. We'll just reproduce what we implemented above in `PyMC3`: a Gaussian prior on the inferred rotation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8acdf0-cedb-4688-8033-1ae115c7b67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def previous_prior(rotation_inferred, period_guess):\n",
    "    \"\"\"\n",
    "    The prior of a given star's rotation rate.\n",
    "    the pymc3 prior is normal in log sigma. oh, this is prior, not log prior!\n",
    "    \"\"\"\n",
    "    sigma = 2 # from rotation rate prior.\n",
    "    rotation_inferred_log = np.log10(rotation_inferred)\n",
    "    mu = np.log10(period_guess)\n",
    "    return np.log(1.0/(np.sqrt(2*np.pi)*sigma))-0.5*(rotation_inferred_log-mu)**2/sigma**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09a01-0b65-44c3-8b15-5696e66fe89c",
   "metadata": {},
   "source": [
    "Here's where things start to get tricky. According to [the references], to see how likely a given set of hyperparameters is — that is, how likely a trend line through our population is — we need to calculate an *updated* prior based on the currently simulated population-level trend. This updated prior is a better prior for each star's rotation rate, because it will (once everything is converged) encode knowledge about the trend. It is the prior inferred from the population of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535fddd-9d53-4427-af35-a5d0e69e30c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def updated_prior(rotation_inferred, m, b, flare_rate, sigma_2):\n",
    "    \"\"\"\n",
    "    The prior of a given star's rotation rate, updated to reflect the contribution from the population-level trend.\n",
    "    \"\"\"\n",
    "    # this is P_alpha\n",
    "    sigma = sigma_2\n",
    "\n",
    "\n",
    "    return np.log(1.0/(np.sqrt(2*np.pi)*sigma))-0.5*(np.log10(rotation_inferred)-f_rotation(m, b, flare_rate))**2/sigma**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ed76c-8a37-4ab3-94e8-3528347ce153",
   "metadata": {},
   "source": [
    "Why do we care about this updated prior? It turns out that this updated prior features into the hierarchical likelihood — the likelihood of the dataset given the current set of parameters (the slope and intercept of the trend line). The computation is as follows.\n",
    "\n",
    "1. For each star, loop over each of its period samples.\n",
    "2. For each sample, calculate the updated prior and the previous prior.\n",
    "3. Take the ratio of these priors. The result is the likelihood for that sample.\n",
    "4. Repeat steps 2-3 for each sample.\n",
    "5. Repeat steps 1-4 for each star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55380d8-cac8-4875-b5cb-7d54be0f1693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def likelihood_hierarchical(samples, m, b, sigma_2, flare_rates_durations_cleaned, rot_rates_constrained):\n",
    "    \"\"\"\n",
    "    The likelihood function of the population-level trend.\n",
    "    \n",
    "    the regular prior was normal in log period\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    total_likelihood = 1.\n",
    "    # looping over whole populatio\n",
    "    for i, rotations_inferred in enumerate(samples):\n",
    "        \n",
    "        # right now looping over a single planet's retrieved rotation rate\n",
    "        likelihood = 0.\n",
    "        flare_rate = flare_rates_durations_cleaned[i]\n",
    "        period_guess = rot_rates_constrained[i]\n",
    "        for rotation_inferred in rotations_inferred:\n",
    "            updated_prior_val = np.exp(updated_prior(rotation_inferred, m, b, flare_rate, sigma_2))\n",
    "            previous_prior_val = np.exp(previous_prior(rotation_inferred, period_guess))\n",
    "            prior_update = updated_prior_val / previous_prior_val\n",
    "            likelihood += prior_update\n",
    "\n",
    "        total_likelihood *= likelihood / len(rotations_inferred)\n",
    "    \n",
    "    return np.log(total_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535ef8b-590f-4aa8-b309-08136282c66f",
   "metadata": {},
   "source": [
    "With all the difficult statistical machinery behind us, we can now calculate the posterior of our hierarchical model: the probability of a given trend line given the data. To calculate it, all we need to do is add the log hierarchical likelihood to the log hyperprior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe583c-c8a8-4563-98cc-43d47f225b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def posterior_hierarchical(hypertheta, samples, flare_rates_durations_cleaned, rot_rates_constrained):\n",
    "    \"\"\"\n",
    "    The posterior distribution of the population-level trend.\n",
    "    \n",
    "    Technically this is the probability function...change!\n",
    "    \"\"\"\n",
    "    m, b, sigma_2 = hypertheta\n",
    "    # likelihood * P_a\n",
    "    \n",
    "    hyperprior_val = hyperprior(hypertheta)\n",
    "    \n",
    "    if not np.isfinite(hyperprior_val):\n",
    "        return -np.inf\n",
    "    \n",
    "    # error maybe in likelihood func\n",
    "    likelihood_val = likelihood_hierarchical(samples, m, b, sigma_2, flare_rates_durations_cleaned, rot_rates_constrained)\n",
    "    \n",
    "    return likelihood_val + hyperprior_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4fcb8-4603-434b-a17f-41990b2119b9",
   "metadata": {},
   "source": [
    "We're now all set up for our MCMC sampling. But where do we start the walkers? Let's instantiate them with a simple regression result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6186f72-322c-4281-9f30-ebeec5ad2761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform regression on the log values\n",
    "\n",
    "rot_rates_gp = np.array(rot_rates_gp)\n",
    "\n",
    "rot_rates_constrained_cleaned = rot_rates_constrained[flare_rates_durations>0.0]\n",
    "rot_rates_gp_cleaned = rot_rates_gp[flare_rates_durations>0.0]\n",
    "flare_rates_durations_cleaned = flare_rates_durations[flare_rates_durations>0.0]\n",
    "\n",
    "flare_rates_durations_sorted = np.sort(np.log10(flare_rates_durations_cleaned))\n",
    "\n",
    "rot_rates_constrained_sorted = rot_rates_constrained_cleaned[np.argsort(np.log10(flare_rates_durations_cleaned))]\n",
    "\n",
    "rot_rates_gp_sorted = rot_rates_gp_cleaned[np.argsort(np.log10(flare_rates_durations_cleaned))]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(np.log10(flare_rates_durations_sorted),\n",
    "                                                                     np.log10(np.mean(rot_rates_gp_sorted, axis=1)))\n",
    "\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db31b01-907d-447d-84f9-8ba107b455f5",
   "metadata": {},
   "source": [
    "Now we're ready to sample the posterior distribution! We'll use the `emcee` package to do so. `emcee` is an MCMC (Markov Chain Monte Carlo) sampler that's used to efficiently sample distributions. It begins with a number of *walkers* that step through parameter space, seeking to flesh out the posterior distribution. The paths that these walkers take are known as *chains*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c67fae-ea58-4ebb-bad9-95cd24877194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_dim = 3\n",
    "n_walkers = 32 \n",
    "n_steps = 1000 \n",
    "\n",
    "\n",
    "initial_point = np.array([slope, intercept, .5])\n",
    "gaussian_scatter_level = 1e-3 # want the walkers to each start at a slightly different location.\n",
    "\n",
    "# add some scatter to the walkers around the initial position.\n",
    "pos = initial_point + gaussian_scatter_level * np.random.randn(n_walkers, n_dim)\n",
    "\n",
    "\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    n_walkers, n_dim, posterior_hierarchical, args=(rot_rates_gp_sorted, flare_rates_durations_sorted,\n",
    "                                                    rot_rates_constrained_sorted)\n",
    ")\n",
    "sampler.run_mcmc(pos, n_steps, progress=True);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf42d5-f893-4f38-b0fc-bce2571ca7e5",
   "metadata": {},
   "source": [
    "Our sampling is complete! Let's take a closer look at how our walkers have explored parameter space. To do so, we can construct \"trace plots,\" which show the path of each walker at each step. These plots are great to quickly assess if something has gone wrong with our sampling. For instance, if we accidentally initialized our walkers outside of our prior range, then they won't be able to move anywhere, and the trace plot will just show horizontal lines all the way through. Incidentally, another clue that this specific problem is occurring is that your posterior is being evaluated suspiciously quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833b842-36f2-4880-885c-4ddf658e4658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_dim, figsize=(10, 3), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = ['m', 'b', 'sigma']\n",
    "\n",
    "for i in range(n_dim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d7ff4-3a56-428e-b20f-97ac1b452927",
   "metadata": {},
   "source": [
    "Nice! It looks like the walkers started out quite close to each other, but then they rapidly moved outward to explore parameter space in all three dimensions.\n",
    "\n",
    "We mentioned that problem with correlated steps earlier, though. One way to check that our sampler has converged is that we've run our chains for a few multiples of the autocorrelation time (the number of steps in between independent samples). Let's assess whether that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f3630-d704-4422-a2d9-3def02157362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau = sampler.get_autocorr_time()\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda493f2-ad17-4a97-b7ef-31581f21e650",
   "metadata": {},
   "source": [
    "Great! Let's reshape our chains for plotting and discard a few multiples of the autocorrelation time. We also thin our chains, because in many sampling schemes, adjacent samples are not actually independent from one another. If we want independent samples (which we do for unbiased estimates!), we must *thin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf00b6-8c1f-40f0-9a3c-8102777017e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "print(flat_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c29d3b-f946-4169-b187-f1fe7eb8569f",
   "metadata": {},
   "source": [
    "Using the `corner` package, we can now visualize the marginal distributions of each parameter and see how they covary with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9e8ac-23bf-444b-a7ef-238ded4948b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b7fd-9a09-41fc-8436-0869e562e2f3",
   "metadata": {},
   "source": [
    "It looks like the slope and intercept parameters are pretty degenerate with one another. This means that the impact on the likelihood by changing `m` a little can also be produced by changing `b` a little. More intuitively put, it means that these parameters are hard to disentangle from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a03e4a-cda9-4042-bd2d-9462cce3139f",
   "metadata": {},
   "source": [
    "Let's plot up the results of all this statistical modeling in the plane we care about: flare rate vs. stellar rotation rate. \n",
    "\n",
    "Loosely, we can plot the error on a single rotation period as the standard deviation of its posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913073a7-dc29-46d4-8a34-4eef025eacb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yerr = np.std(rot_rates_gp_sorted, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5ea00-cfb0-48b0-8e48-c5501b93fe23",
   "metadata": {},
   "source": [
    "Next, we can draw a bunch of random samples from our hyperparameter chain. Using these to plot trend lines will tell us the approximate \"spread\" of our trend inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6f038-db5b-4de8-b036-7473ba72d9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inds = np.random.randint(len(flat_samples), size=100)\n",
    "for ind in inds:\n",
    "    sample = flat_samples[ind]\n",
    "    plt.plot(np.log10(flare_rates_durations_sorted), (np.dot(np.vander(np.log10(flare_rates_durations_sorted), 2), sample[:2])), \"C1\", alpha=0.1)\n",
    "plt.errorbar(np.log10(flare_rates_durations_sorted),np.log10(np.mean(rot_rates_gp_sorted, axis=1)),\n",
    "            yerr=yerr/np.mean(rot_rates_gp_sorted, axis=1) / np.log(10),fmt='.')\n",
    "# plt.plot(x0, m_true * x0 + b_true, \"k\", label=\"truth\")\n",
    "# plt.\n",
    "plt.legend(fontsize=14)\n",
    "# plt.xlim(0, 10)\n",
    "plt.xlabel(\"log flare rate per day\")\n",
    "plt.ylabel(\"log period (days)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c001c5-8408-4020-aa0b-ac495eacbdbf",
   "metadata": {},
   "source": [
    "Recall from earlier plots, though, that some of those rotation rate inferences did not look very Gaussian or symmetric. So, symmetric error bars like the ones we've drawn above can be a bit disingenuous. Let's plot the actual distributions, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6edf9ce-4c9d-452e-8f53-cea7b540acd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = pd.DataFrame({'flare':np.repeat(np.log10(flare_rates_durations_sorted),2000),\n",
    "              'rot':np.log10(rot_rates_gp_sorted.flatten())})\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(\n",
    "    data=dat,\n",
    "    x=\"flare\", y=\"rot\", hue=True,\n",
    "    hue_order=[True, False], split=True, scale_hue=12,color='teal',ax=ax, common_norm=False\n",
    ")\n",
    "ax.legend_ = None\n",
    "ax.set_xticks([0.,10, 21], [0.6, 0.7, 0.8])\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlabel('Log flare rate (per day)', fontsize=18)\n",
    "ax.set_ylabel('Log rotation date (days)', fontsize=18)\n",
    "\n",
    "inds = np.random.randint(len(flat_samples), size=100)\n",
    "for ind in inds:\n",
    "    sample = flat_samples[ind]\n",
    "    ax.plot(20 * (np.log10(flare_rates_durations_sorted) -0.58708446)/ (0.83866827 - 0.58708446), (np.dot(np.vander(np.log10(flare_rates_durations_sorted), 2), sample[:2])), \"C1\", alpha=0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9414c5-5dc5-4a5c-bf3c-f7ea5057128e",
   "metadata": {},
   "source": [
    "Nice! This plot gives us a better sense of where each stellar rotation rate's uncertainty is concentrated. To the left of each line, the distribution plotted shows the density of rotation samples for that single star. \n",
    "\n",
    "This plotting approach is also more accurate with respect to the trend lines, as those were calculated based on the full posterior distribution of each stellar rotation rate, instead of a single, symmetric uncertainty value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db7594-b52b-4c60-81f7-847f03b6935b",
   "metadata": {},
   "source": [
    "So it looks like we can decently fit a trend line to these rotation rates. A lot of these trends, though, look pretty flat. This begs the question: How would a flat line perform against the data? Do we need the slope parameter at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68cd9f-0bf4-469e-9aad-512415fbfe68",
   "metadata": {},
   "source": [
    "## Exercise 1<a name=\"exercise_1\"></a>\n",
    "Implement the function `f_rotation_flat`. Like the function `f_rotation`, it calculates the population-level trend describing the relationship between the flare rate and the rotation rate. However, in `f_rotation_flat`, there is *no* relationship between the flare rate and the rotation rate. That is, the trend line is just a flat line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43baa593-1c6a-4963-9d8e-678f50a3986d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def f_rotation_flat(b, flare): # just a log-linear functional form\n",
    "    \"\"\"\n",
    "    this is the function that returns the LOG rotation rate.\n",
    "    \"\"\"\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db5d12-a558-4c6f-bf8a-4a14c7c5ce2f",
   "metadata": {},
   "source": [
    "## Performing hierarchical Bayesian modeling of a flat line<a name=\"performing_flat\"></a>\n",
    "With our new `f_rotation_flat` function implented, much of our modeling will stay the same. All we have to do us make sure we use that new equation to describe the trend line and pass fewer parameters through the rest of our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbde08-d908-4b56-865e-45bb1911c859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def updated_prior_flat(rotation_inferred, b, flare_rate, sigma_2):\n",
    "    # this is P_alpha\n",
    "\n",
    "    sigma = sigma_2\n",
    "    return np.log(1.0/(np.sqrt(2*np.pi)*sigma))-0.5*(np.log10(rotation_inferred)-f_rotation_flat(b, flare_rate))**2/sigma**2 \n",
    "\n",
    "@njit\n",
    "def likelihood_hierarchical_flat(samples, b, sigma_2, flare_rates_durations_cleaned, rot_rates_constrained):\n",
    "    \"\"\"\n",
    "    the regular prior was normal in log period\n",
    "    \"\"\"\n",
    "    total_likelihood = 1.\n",
    "    # looping over whole population\n",
    "    for i, rotations_inferred in enumerate(samples):\n",
    "        # right now looping over a single planet's retrieved rotation rate\n",
    "        likelihood = 0.\n",
    "        flare_rate = flare_rates_durations_cleaned[i]\n",
    "        period_guess = rot_rates_constrained[i]\n",
    "        for rotation_inferred in rotations_inferred:\n",
    "            updated_prior_val = np.exp(updated_prior_flat(rotation_inferred, b, flare_rate, sigma_2))\n",
    "            previous_prior_val = np.exp(previous_prior(rotation_inferred, period_guess))\n",
    "            prior_update = updated_prior_val / previous_prior_val\n",
    "            likelihood += prior_update\n",
    "\n",
    "        total_likelihood *= likelihood / len(rotations_inferred)\n",
    "    \n",
    "    return np.log(total_likelihood)\n",
    "\n",
    "@njit\n",
    "def posterior_hierarchical_flat(hypertheta, samples, flare_rates_durations_cleaned, rot_rates_constrained):\n",
    "    b, sigma_2 = hypertheta\n",
    "    # likelihood * P_a\n",
    "    \n",
    "    hyperprior_val = hyperprior_flat(hypertheta)\n",
    "    \n",
    "    if not np.isfinite(hyperprior_val):\n",
    "        return -np.inf\n",
    "    \n",
    "    likelihood_val = likelihood_hierarchical_flat(samples, b, sigma_2, flare_rates_durations_cleaned, rot_rates_constrained)\n",
    "    \n",
    "\n",
    "    return likelihood_val + hyperprior_val\n",
    "\n",
    "\n",
    "# we have all of these. now do a hyperprior.\n",
    "@njit\n",
    "def hyperprior_flat(hypertheta):\n",
    "    b, sigma_2 = hypertheta\n",
    "    if not (-10 < b < 10 and sigma_2 > 0.2):\n",
    "        return -np.inf\n",
    "\n",
    "    \n",
    "    #gaussian prior on sigma_2\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "    return np.log(1.0/(np.sqrt(2*np.pi)*sigma))-0.5*(sigma_2-mu)**2/sigma**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5491033-5759-4b2a-ab12-0307ed5aca8d",
   "metadata": {},
   "source": [
    "Once again, we perform our `emcee` sampling. This time, though, we only have two dimensions. We'll start our walkers at the mean of our rotation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd9bc2-495b-4e0c-9e8d-65fe29c877cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_dim = 2\n",
    "n_walkers = 32 \n",
    "n_steps = 1000 \n",
    "\n",
    "\n",
    "initial_point = np.array([np.mean(np.log10(rot_rates_gp_sorted)), .5]) # incorporate as an array and not a list so that we can add to it, etc.\n",
    "gaussian_scatter_level = 1e-3 \n",
    "\n",
    "# add some scatter to the walkers around the initial position.\n",
    "pos = initial_point + gaussian_scatter_level * np.random.randn(n_walkers, n_dim)\n",
    "\n",
    "\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    n_walkers, n_dim, posterior_hierarchical_flat, args=(rot_rates_gp_sorted, flare_rates_durations_sorted,\n",
    "                                                    rot_rates_constrained_sorted)\n",
    ")\n",
    "sampler.run_mcmc(pos, n_steps, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54812a5a-2f81-4ab6-922d-c7f734ee64bb",
   "metadata": {},
   "source": [
    "Let's check the trace plots to make sure nothing funky happened in the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86137a-4adf-49f8-a718-dad45232a789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_dim, figsize=(10, 3), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = ['b', 'sigma']\n",
    "\n",
    "for i in range(n_dim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7cf69-7ecd-4a78-976c-a6a29fe155d0",
   "metadata": {},
   "source": [
    "Things are looking pretty good. Again, we calculate our autocorrelation time and remove a few multiples of that time in steps, then plot the corner plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268b40a-99c6-4b16-ae4b-97ecb026e149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau = sampler.get_autocorr_time()\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e65a7f-f492-4500-af13-7c35084191ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_samples_flat = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "print(flat_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb2859-b5ca-4443-b9a6-61d3567e7fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    flat_samples_flat, labels=labels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49869a-ece3-442d-b976-4d22c673d54e",
   "metadata": {},
   "source": [
    "Looks great! Now let's plot the trend line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb362f41-b8c1-4625-8bb3-ecce8d30acde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = pd.DataFrame({'flare':np.repeat(np.log10(flare_rates_durations_sorted),2000),\n",
    "              'rot':np.log10(rot_rates_gp_sorted.flatten())})\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(\n",
    "    data=dat,\n",
    "    x=\"flare\", y=\"rot\", hue=True,\n",
    "    hue_order=[True, False], split=True, scale_hue=12,color='teal',ax=ax\n",
    ")\n",
    "ax.legend_ = None\n",
    "ax.set_xticks([0.,10, 21], [0.6, 0.7, 0.8])\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlabel('Log flare rate (per day)', fontsize=18)\n",
    "ax.set_ylabel('Log rotation date (days)', fontsize=18)\n",
    "\n",
    "inds = np.random.randint(len(flat_samples_flat), size=100)\n",
    "for ind in inds:\n",
    "    sample = flat_samples_flat[ind]\n",
    "    ax.plot(20 * (np.log10(flare_rates_durations_sorted) -0.58708446)/ (0.83866827 - 0.58708446), (np.dot(np.vander(np.log10(flare_rates_durations_sorted), 2), sample[:2])), \"C1\", alpha=0.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fc8fa-042a-40f6-97d6-9a06a800a880",
   "metadata": {},
   "source": [
    "These flat lines also seem to do pretty well at describing the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77c4ea-d8bb-4d4f-87b4-b27f10cf7b94",
   "metadata": {},
   "source": [
    "# Model comparison<a name=\"comparison\"></a>\n",
    "We now have two competing models for the relationship between flare rate and rotation rate: a flat line and a linear trend. Which one is \"right\"?\n",
    "\n",
    "Philosophically, Bayesian modeling can't tell us what model underlies a given dataset. But it *can* tell us what the data prefer. That is, we can perform a model comparison exercise that tells us whether the data quality warrant a complex model or a simpler one.\n",
    "\n",
    "This idea is based on the idea of Occam's Razor. Loosely, we don't want to use a complex idea to explain a concept when a simpler one does just as well. Mathematically, we can express this with the Bayesian Information Criterion:\n",
    "\n",
    "BIC = $-2\\mathcal{L} + k\\log(N)$,\n",
    "\n",
    "for log likelihood $\\mathcal{L}$, number of parameters $k$, and number of data points $N$. This quantity captures how well a given model performs (via the likelihood), but it penalizes the model for however many parameters it uses. The difference between the BIC for two different models — the $\\Delta $BIC — can therefore tell us if one model performs better than another in a way that isn't \"cheating\" by using too many parameters to create a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caf73d-c23b-42d8-9470-1b95315c29ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2<a name=\"exercise_2\"></a>\n",
    "Implement a function to calculate the Bayesian Information Criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd0f1e-551e-4f36-b17f-9eb95456895b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_bic(log_likelihood, n_points, n_params):\n",
    "    \"\"\"\n",
    "    calculates the Bayesian Information Criterion.\n",
    "    \"\"\"\n",
    "    bic = -2 * log_likelihood + np.log(n_points) * n_params\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1edd9f-5b6b-4572-983b-62473bb3e989",
   "metadata": {},
   "source": [
    "## Performing the model comparison<a name=\"perform_comparison\"></a>\n",
    "Now that we can calculate the BIC, let's perform the calculation on each model and take the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4355ed-850e-41ab-b43b-145506600f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "n_points = len(rot_rates_gp_sorted)\n",
    "\n",
    "\n",
    "# trend line likelihood\n",
    "flat_chain_mean = np.mean(flat_samples, axis=0)\n",
    "log_likelihood_trend = likelihood_hierarchical(rot_rates_gp_sorted, flat_chain_mean[0],\n",
    "                                               flat_chain_mean[1],\n",
    "                                               flat_chain_mean[2],\n",
    "                                               flare_rates_durations_sorted,\n",
    "                                                    rot_rates_constrained_sorted)\n",
    "\n",
    "\n",
    "# flat line likelihood\n",
    "flat_chain_flat_mean = np.mean(flat_samples_flat, axis=0)\n",
    "log_likelihood_flat = likelihood_hierarchical_flat(rot_rates_gp_sorted, \n",
    "                                                   flat_chain_flat_mean[0], \n",
    "                                                   flat_chain_flat_mean[1], \n",
    "                                                   flare_rates_durations_sorted,\n",
    "                                                    rot_rates_constrained_sorted)\n",
    "\n",
    "\n",
    "# do delta BIC calculation\n",
    "bic_trend = calc_bic(log_likelihood_trend, n_points, 3)\n",
    "bic_flat = calc_bic(log_likelihood_flat, n_points, 2)\n",
    "\n",
    "delta_bic = bic_trend - bic_flat\n",
    "delta_bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb309fd9-464c-4a0a-92e6-06e9625b3bdb",
   "metadata": {},
   "source": [
    "We've calculated a low $\\Delta$BIC. In general, we look for a $\\Delta$BIC greater than 2 before we consider the data to prefer one model over the other. The rule of thumb is that anything less than 2 is \"not worth more than a bare mention\" (per [Kass & Rafter 1993](https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572?casa_token=79jMtuYH2oIAAAAA:nGzwyhOOBoVciRDbetSj3gTMy1DMmb86Ubavmd456g5eYX0eM7ftmVO_w8SEJ6laqomgXvEnnCmGeg)). The intuition here is that the trend line only does a little bit better at fitting the data than a flat line, and it needs a whole extra parameter to do so. Therefore, we might as well say that a flat line describes the data as well as a trend line.\n",
    "\n",
    "That's a long way of saying that the trend we found was not statistically significant — even though it aligned with our predictions!\n",
    "\n",
    "Importantly, that's not to say that our approach is flawed, or that there isn't an actual underlying trend. It may be that further observations could help us differentiate between a trend and a flat line.\n",
    "\n",
    "Note: full Bayesian model comparison involves calculating a quantity known as the Bayesian evidence. This is usually a tricky thing to do, and in our case it likely isn't warranted given how small the $\\Delta$BIC is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c635047-b2bc-4494-b9ef-da6e36e473f8",
   "metadata": {},
   "source": [
    "# Additional exercises<a name=\"more_exercises\"></a>\n",
    "3. Perform the above but for a half-tailed Gaussian distribution. Is a flat line still a better fit?\n",
    "4. Based on the Gelman-Rubin statistic, it may be that our Gaussian process isn't fully converged. Explore whether the convergence statistics improve with more draws from the posterior distribution or by changing the tuning-related parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf54b98-de31-4a93-b3e1-7cdc7951b3cb",
   "metadata": {},
   "source": [
    "# Resources<a name=\"resources\"></a>\n",
    "- Gaussian Process review: [Rasmussen & Williams 05](https://link.springer.com/content/pdf/10.1007/b100712.pdf#page=71)\n",
    "- Application of Gaussian Processes to stellar rotation: [Angus+19](https://academic.oup.com/mnras/article/474/2/2094/4209242)\n",
    "- Gaussian Processes in astronomy review: [Aigrain & Foreman-Mackey 23](https://www.annualreviews.org/doi/abs/10.1146/annurev-astro-052920-103508))\n",
    "- `exoplanet` tutorial on stellar variability: https://gallery.exoplanet.codes/tutorials/stellar-variability/\n",
    "- `emcee` tutorial on line-fitting, with sampling introduction: https://emcee.readthedocs.io/en/stable/tutorials/line/\n",
    "- guide for Bayesian inference in astronomy: https://arxiv.org/pdf/2302.04703.pdf\n",
    "- review on model-fitting: https://arxiv.org/abs/1008.4686\n",
    "- `celerite2` documentation: https://celerite2.readthedocs.io/en/latest/\n",
    "- `pymc3` documentation: https://www.pymc.io/projects/docs/en/v5.8.1/learn/core_notebooks/pymc_overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22577b-569b-4e42-8850-b621b183b951",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Last updated:** July 2024 <br>\n",
    "\n",
    "For support, please contact the Archive HelpDesk at archive@stsci.edu.\n",
    "\n",
    "***\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/>\n",
    "\n",
    "[Return to top of page](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celerite_docs3",
   "language": "python",
   "name": "celerite_docs3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
